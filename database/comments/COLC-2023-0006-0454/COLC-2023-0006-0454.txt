I think a work created by a Generative AI model is very complicated to judge and should take a heavily nuanced approach to licensing<br/><br/>I would judge the &quot;copywrite&quot; of a work under these parameters:<br/><br/>Under what conditions were the model trained:<br/>What % of the training data belonged to an individual copywrite owner.<br/><br/>in cases where there are many many copywrite owners involved in the data then things get more complicated and should be taken into account.<br/><br/>Additionally models can work on &quot;seed data&quot; where the input could be a copywrited work run through a model that transforms it in some way.<br/><br/>Instead of a one party fits all system there should be a set of scaffolding upon which future details and &quot;what is acceptable&quot; can be built.<br/><br/>For example,<br/>if an author trains a model on only books that author wrote<br/>And uses it to write a new book without further human input.<br/><br/>That copywrite should be passed through because it is a representation of the author itself.<br/><br/>However if a model is trained on every book in the library of congress and then writes a book without any further human input, then there is no &quot;sole&quot; copywrite owner but instead many many owners.<br/><br/>A good way to think of it is you had a large community of people (thousands of people) make a painting together.<br/><br/>it was self organized and there was no larger &quot;guider&quot; in this painting.<br/>Each person only made a single stroke.<br/><br/>Who owns the &quot;finished painting&quot;?<br/><br/>Training models on other copywritten work is a very similar situation.<br/>Each piece of data trained on is like a little stroke in a much larger work of art.<br/><br/>My opinion for the base scaffolding is:<br/>Copywrite carries through the model.<br/><br/>The output of a generative AI model contains the legal copywrite of every individual piece of training data used to form that model.<br/><br/>With this base piece we can get around lots of thorny questions:<br/>The author who did everything by scratch gets his own representation of himself to contain his copywrite.<br/><br/>The author who published a sketch they made on twitter that was gobbled up by a random AI has a legitimate legal copywrite to the work created by that random AI<br/><br/>The second level is:<br/>A special fair use applies to data for generative models.<br/><br/>Fair use as it stands needs more framework around how it is determined with results to a generative AI<br/><br/>Additional application aspects is:<br/>How much did a copywritten work impact the training of the model.<br/>Quantifiably this is:<br/>What % of the training set did this copywritten work make up.<br/>How many times was this particular copywritten work trained on.<br/>Is this copywritten work &quot;over populated&quot; compared to any other copywritten work within the dataset.<br/><br/>LLMs that use million of pieces of data are likely not recognizable from an individual&#39;s work,  this should be respected.<br/><br/>But at the same time some works that can be over-represented because they are either famous or spread out more.<br/><br/>The likeness of a president will appear much more in a model trained on social media than a regular individual.<br/><br/>That president should have a more legitimate claim to fair use not applying because they were represented more than the average member of the dataset.<br/><br/>The third (and most complicated layer):<br/>Consent.<br/><br/>In the past (and up until very recently) sharing a work was an explicit action.<br/><br/>If you were an author it was only viewed if it was explicitly shared / published.<br/>The point of copywrite was to prevent someone else from taking your work (or significant portions of it) and claiming it as their own without putting in any level.<br/><br/>You had a level of consent in the process where your work could not be used in certain ways [excluding fair use] without your permission.<br/><br/>With social media and now these models, it is very very easy for your work to get gobbled up into an AI where you do not have consent on how it is used.<br/><br/>There should be a method available where a person can exercise consent.<br/>Be that explicitly waive their copywrite claim to the result of the model<br/>Or that for a model result to be copy writeable, every source of copywritten data must have provably provided consent.<br/><br/>So to sum it up:<br/>Models carry through the copywrite of the training data.<br/><br/>This allows infringement to be made on how the model output is used,<br/>(artist can say that the selling of an autogenerative work violates their copywrite if their data is used in training it)<br/><br/>Fair use applies:<br/>If the artist whose copywrite is violated is a tiny part of the training data then it can be considered fair use.<br/>However if multiple artist (with standing) as a group claim together then that could potentially no longer be considered fair use.<br/><br/>Consent is required as exceptions:<br/>An artist can waive their copywrite and the model no longer needs to carry that copywrite through in its output.<br/><br/>Thanks!<br/>Good luck y&#39;all!