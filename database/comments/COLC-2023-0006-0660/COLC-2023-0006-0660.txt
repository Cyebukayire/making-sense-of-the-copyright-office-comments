&quot;Generative&quot; AI systems can&#39;t generate images without being provided with hundreds of thousands of samples&mdash;which are encoded into their model through a process called &quot;training.&quot; The model will then only produce material that is similar to its &quot;training dataset.&quot; Unfortunately, there is little to no transparency about the origins of these training datasets, and many image-&quot;generation&quot; models will partially reproduce watermarks of stock-photo companies and recognizable signatures of human artists, many of whom were unaware their art would be included in the datasets.<br/><br/>Likewise, the GitHub Copilot model trained by Microsoft will attempt to insert copyright notices and license headers from popular open source (copyleft) and source-available (all rights reserved) projects hosted on GitHub, despite initially not crediting the authors of those projects for their non-consenting contributions to the model&#39;s training dataset.<br/><br/>As such, I hope it&#39;s clear that &quot;Generative AI&quot; doesn&#39;t generate, it replicates. The jargon of AI researchers is, intentionally or otherwise, obfuscating the real meaning. Modern AI models are a lossy algorithm, (like MP3 and JPEG,) and the material they &quot;generate&quot; is only as original as the data set it was composited from.