Q 8.4:<br/>I work on automated detection of railway defects. Deep learning has led to significant improvements in recall and precision compared to traditional computer vision methods, but requires large quantities of data. It&#39;s typical for productionized detector models to be pretrained on ~millions of web images, learning generic features like edges/textures, before being fine-tuned to a specific task with ~thousands of images available. <br/><br/>I don&#39;t believe the substantiality factor weighs against a finding of fair use for this, since:<br/><br/>* Kelly v. Arriba Soft: &quot;If the secondary user only copies as much as is necessary for his or her intended use, then this factor will not weigh against him or her&quot;. The amount of information used during model fitting is large, but not needlessly so. Deep learning with small data is an active area of research, however generally refers to limiting only the task-specific dataset. Web-scale pretraining remains a necessity for many uses and, as for some tasks the only known alternatives evolved on 3.7bil years&#39; worth of data, it&#39;s not a given that the need will decrease with future developments.<br/><br/>* Sega v. Accolade: &quot;Accolade, a commercial competitor of Sega, engaged in wholesale copying of Sega&#39;s copyrighted code as a preliminary step in the development of a competing product&quot; yet &quot;where the ultimate (as opposed to direct) use is as limited as it was here, the factor is of very little weight&quot;. Full works are transiently utilized during model development, but a model&#39;s weights (particularly for smaller models) may ultimately have under a byte of information about a given work and none of its fixed expressive content.<br/><br/>* Campbell v. Acuff-Rose Music: &quot;The more transformative the new work, the less will be the significance of other factors&quot;, defined as &quot;whether the new work merely &#39;supersede[s] the objects&#39; of the original creation [...] or instead adds something new&quot;. For instance, using images to fit a model that can remove backgrounds adds new purpose and does not at all displace the original works. A model like Google Translate may partly displace the translations it was fit on - but also allows instant translation of new text, which static translations can&#39;t, so doesn&#39;t merely supersede without adding something new.<br/><br/>Q 9.2:<br/>It&#39;s already standard practice to respect machine-readable opt-outs, as is required for commercial data mining by the EU DSM Directive. robots.txt has long been the de facto standard for restricting automated processing of publicly accessible content, with per-file and per-bot granularity. More recently:<br/>* a &quot;NoAI&quot; HTML meta tag was adopted by content sharing sites including ArtStation and Sketchfab<br/>* &quot;IPTC metadata&quot; on media allows specifying whether and for what purposes data mining is prohibited (e.g: DMI-PROHIBITED-EXCEPTSEARCHENGINEINDEXING)<br/><br/>In Field v. Google Inc, similar mechanisms were taken as an &quot;implied license&quot; for rehosting a snapshot of a site when &quot;Google reasonably interpreted absence of meta-tags as permission to present &#39;Cached&#39; links to the pages of Field&#39;s site&quot;.<br/><br/>Q 14:<br/>Fitting statistical models to data (&quot;training AI&quot;) is prevalent across many industries for a range of beneficial and generally uncontroversial purposes. Various examples I know of:<br/><br/>* Protein folding: alphafold.ebi.ac.uk<br/>* Hurricane/weather forecasting: wired.com/story/ai-hurricane-predictions-are-storming-the-world-of-weather-forecasting<br/>* Voice transcription: openai.com/research/whisper<br/>* Malware detection: microsoft.com/en-us/security/blog/2017/08/03/windows-defender-atp-machine-learning<br/>* Drug discovery: paperswithcode.com/task/drug-discovery<br/>* Defect detection: github.com/lmomoy/lf-yolo<br/>* Spam filtering: wired.com/2015/07/google-says-ai-catches-99-9-percent-gmail-spam<br/>* Language translation: github.com/marian-nmt/marian-dev<br/>* Fraud prevention: thebanker.com/AI-and-the-new-age-of-fraud-detection-1506931228<br/>* Web search: blog.google/products/search/search-language-understanding-bert<br/>* Road traffic prediction: deepmind.com/blog/traffic-prediction-with-advanced-graph-neural-networks<br/>* Data compression: github.com/tensorflow/compression<br/>* Tumor segmentation: github.com/cchen-cc/ma-sam<br/>* OCR: github.com/GXYM/TextBPN-Plus-Plus<br/>* Image denoising/restoration: github.com/megvii-research/NAFNet<br/><br/>Many of the above benefit from or are made feasible by web-scale pretraining/foundation models. None have had an apparent negative impact on creation of creative works. <br/><br/>I&#39;d argue that commenters objecting to &quot;AI&quot; or &quot;training&quot; don&#39;t actually take issue with the above - but rather with the use of generative tools to create infringing works. These infringing works can be handled by lawsuits, as with human-created infringing works. Restricting training would be simultaneously insufficient (tools like img2img/voice cloning don&#39;t need training on the work to copy) and overly broad (rendering common unproblematic uses of machine learning only feasible to companies with large data moats).<br/>