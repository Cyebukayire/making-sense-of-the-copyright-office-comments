While there are many different image generation models, each with their own technologies and architecture, modern image generators all utilize deep learning, a subset of machine learning techniques that are based on artificial neural networks with representation learning. Let’s define some terms, machine learning is the field of AI that aims to teach computers to learn and make decisions without being explicitly programmed for each task. An artificial neural network is a collection of nodes, or neurons, that connect to form layers, their architecture being inspired by the human brain, with nodes able to send signals between each other. This artificial neural network uses representation learning, a concept where a model automatically discovers and extracts meaningful features from raw data without explicitly being told what to look for. The “deep” in deep learning refers to the depth, or complexity of the layers in the neural network. Now that our terms are defined, let’s look at an example. If I gave you different sets of images and told you to identify what was in the images, you would have no problem doing it. But what if I gave you a sheet of paper filled with code? This is how AI sees images, just pixel values for red, green, and blue. You would just have to make a guess. Which is what the computer does too, at first. You could go through a thousand rounds of this and never get better at it, but the computer would figure out a method that works eventually.



To understand that one arrangement of pixels is a pea, and another arrangement is a tomato, the AI needs metrics that help separate these images in mathematical space. At first, this is done by hand by a programmer. Let’s say we want the model to measure the amount of yellow in the image, it will put the banana and the pea opposite of each other in a one-dimensional space. But what if we get an image of a fresh, green tomato? Color is no longer enough to differentiate between them, so let’s add an axis for size. We now have a two-dimensional space with the yellow objects to the right, and the large objects at the top. But when we put in a million images of tomatoes and peas, we could come across a massive pea. We would need another variable then, let's say the presence of a stem. Now we have a three-dimensional space that our images should theoretically fall into somewhere. But what if we want our model to recognize not just peas and tomatoes, but *everything* else? We need a lot more variables. This process of adding manual rules is time-consuming and open to human bias, so what if we had the computer find its own metrics? That’s exactly what deep learning algorithms do as they go through all the training data. They find variables that help improve their performance on a task, and in the process, they build out a mathematical, multi-dimensional space called latent space. These hundreds of axis would contain variables humans wouldn’t even have names for, and we certainly can’t comprehend more than 3 dimensions. This leads to the neural network of an AI being referred to as a “black box” as we can’t understand exactly how they function; however, the result is that the space for tomatoes has meaningful clusters of the characteristics that make up a tomato. Similar concepts are stored near each other, so it has a space for snow, and a space for globes, and has snow globes somewhere in the middle. Any particular location or point in this space holds what can be thought of as a “recipe” or “instructions” for creating a certain image. This “recipe” is called the latent image patch, and your text prompt is what navigates you to that location. But how do we translate this location in computer-space to an actual image?



There are a few different methods to do this, but today we will look at diffusion, which is what the most popular image generators such as DALL-E, MidJourney, and DreamStudio use. Diffusion uses a “de-noising” algorithm to remove noise from an image and return it to its original state. Programmers did this by having an AI add ten percent noise to an image, and then learn how to reverse the process. The computer adds noise to the image, attempts to remove it, and then compares its solution to the de-noised image, the “correct answer.” Eventually, it became proficient at removing ten percent of noise from images. So, they increased it to twenty percent, and it quickly mastered that too. Eventually, they increased the noise to one hundred percent and told the model to return to the original image. The AI generated an image that didn’t exist before, it was nowhere to be found in the training data. When you use an image generator, your image will start off as this one hundred percent random noise. Your text prompt serves as the “correct answer”, telling the model what the final image should look like. As the model attempts to it gradually transforms the noise into an image it uses another model to ensure the generated image matches the text prompt at each point in the diffusion.

