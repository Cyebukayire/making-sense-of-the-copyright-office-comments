Re: COLC-2023-0006-9044 &quot;Comment from Getty Images (US), Inc&quot; <br/><br/>&gt; &quot;[Pg5] latent diffusion models developed and licensed by Bria,5 which are trained on licensed content and are marketed as safe for commercial use.&quot;<br/><br/>That Bria used licensed content is true, but not the whole truth: the model is based on Stable Diffusion 1.5 with OpenAI&#39;s CLIP ViT-L/14 encoder, both developed with web-scraped data. Attached image 1 demonstrates that the model retains some information from protected works. Adobe and Getty&#39;s image generators are similarly based on Nvidia&#39;s Edify, also developed with a mix of data (arxiv.org/pdf/2211.01324.pdf).<br/><br/>Tuning an open-source model on licensed data, putting a filter in front, then offering legal indemnity does genuinely make for a commercially safer service, but shouldn&#39;t be mistaken for developing a model using *only* what creators opted into training. To my knowledge, the latter has not yet been achieved for any capable LLM or image diffusion model.<br/><br/>&gt; &quot;[Pg17] there is an established path available for licensing copyrighted works for use in training generative AI Models. Accordingly, the loss of license revenue from that market [...]&quot;<br/><br/>I had tried to obtain a license from Getty to develop a model that can describe the contents of photos for visually impaired users. Getty repeatedly told me that they don&#39;t license images for machine learning (as in attached image 2). If there is an established process, their sales department can&#39;t find it and there was no indication of it on their website.<br/><br/>I believe what&#39;s actually meant is that they have partnered with certain companies. This leaves no path for individuals or open-source developer groups.<br/><br/>In contrast, relevant code was remarkably easy to find; there&#39;s a strong culture of releasing free and open-source code, allowing everyone to contribute and build upon each others work. Papers documenting algorithms and experiments are also freely available. arXiv.org (Cornell University&#39;s open-access preprint repository) is *the* place for AI papers, opposed to paywalled journals.<br/><br/>&gt; &quot;[Pg18] Robot.txt[sic] is not a viable means of opt-out. Robot.txt[sic] is applied at a webserver (i.e., website) level versus the content level.&quot;<br/><br/>To clarify, robots.txt can specify certain files - &quot;website level&quot; doesn&#39;t mean on/off for an entire website at once. But it&#39;s true that metadata embedded directly into the image is more convenient for images rehosted in many places.<br/><br/>&gt; &quot;[Pg18] IPTC metadata [...] identifies the copyright owner and &ldquo;Credit&rdquo; [...] and there should be a presumption that authorization is needed, especially, but not only, if a copyright notice exists.&quot;<br/><br/>The IPTC standard defines tags for prohibiting data mining, such as DMI-PROHIBITED-AIMLTRAINING. To my understanding, it&#39;s these tags that are intended to signal that data mining is prohibited, not whether the author&#39;s details field has been filled.<br/><br/>&gt; &quot;[Pg19] 9.5. In cases where the human creator does not own the copyright-for example, because they have assigned it or because the work was made for hire- should they have a right to object to an AI model being trained on their work? [Getty&#39;s response:] We do not believe that copyright law should be expanded such that [...]&quot;<br/><br/>There&#39;s tension between this response and Getty&#39;s stance that people training models &quot;need to obtain affirmative consent&quot; to protect human creators. Is an agreement made prior to generative AI&#39;s existence really affirmative consent to training? Is it right for a company that never received affirmative consent to training from the work&#39;s creator to nonetheless go on to sell affirmative consent for that work?<br/><br/>Some companies that believe their licenses already allow such usage have offered a &#39;goodwill gesture&#39;, like X/Twitter&#39;s revenue program, to show they are voluntarily passing on some profit to creators. My cynical view is that, if creators have no leverage due to being assumed to have already given up those rights, there&#39;s little permanence to such PR moves and they do not meaningfully protect human creators in the long run.<br/><br/>&gt; &quot;[Pg21] there is no credible argument that licensing costs will inhibit innovation. The multi-billion-dollar scale of investment [...] accommodates the cost of obtaining licenses&quot;<br/><br/>This shows complete disregard for the huge amount of AI progress that is made at the level of individual researchers, open-source developer groups, academia, and smaller companies. These do not have multi-billion dollar budgets, and Getty opposes open-source models being offered any exemptions.<br/><br/>The companies that integrate an existing AI model into their site cannot do so in isolation and are usually not the ones driving progress. In an environment that benefits massively from fast iteration and low barrier of entry for contributing and trying out ideas, most of which fail, a requirement to pay large fees in advance of training is not viable and may entirely kill the US&#39;s competitive edge in the field.