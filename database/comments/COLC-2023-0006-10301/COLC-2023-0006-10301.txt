Dear Copyright Office:<br/><br/>First, I wish to state I am John Lopez, member of the WGA&#39;s AI working group, and I wrote this text on my own without aid of any AI system. I&#39;d like to supplement my previous comment with additional links to research and responses to the public comments of OpenAI, Microsoft, Google, and Meta, as well as VC firms like Andreessen Horowitz with a financial stake in the matter. While I wish I could dissect all their comments, I don&#39;t have the time, and you kind people have read enough. <br/><br/>I wish to point out a blatant falsehood in OpenAI&#39;s public comment that illustrates the broader points in my own public comment. In their comment, OpenAI claimed: &quot;Despite a common and unfortunate misperception of the technology, the models do not store copies of the information that they learn from.&quot;  This is false, and OpenAI knew it was false when they wrote this.  That information, including work protected by copyright, is stored in the training weights of an AI model. Furthermore, when OpenAI submitted this comment, researchers at DeepMind had already documented and informed them of adversarial attacks that could prompt ChatGPT to output its training data verbatim. These researchers verified this was real world training data and included material protected by copyright. In fact, they estimate they could have extracted almost 2GB worth of data with an appropriate attack. You can find an explanation of the researchers&#39; results and a link to the paper here: https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html  -- the same is also true of diffusion models and here is another DeepMind paper documenting it: https://www.usenix.org/system/files/usenixsecurity23-carlini.pdf<br/><br/>But sadly, even DeepMind&#39;s parent company, Google makes the claim in their public comment that ingesting training data is just like human learning. Again, this is a false assertion, framed so that they can get around having to license or pay for training data. However, while these machines are complex and impressive, they do no work like the human mind. They do not learn from or generate material the way a human author does. Copying a copyright protected text, turning it into training tokens, and feeding it into a machine-learning algorithm so that it is stored within an AI neural net&#39;s weights should not be given the same fair use protection that a human enjoys. Here are two additional papers from independent AI researchers that explain how LLMs fail to work like a human mind and only generate outputs DERIVED from their training data: 1) the first explains the &quot;reversal curse&quot; failure of LLMs: https://arxiv.org/pdf/2309.12288.pdf  -- the other 2) explains that LLMs do not develop robust abstract reasoning like a human does: https://arxiv.org/pdf/2311.09247.pdf  Again, the point is that these technologies basically memorize and extract interpolated training data when prompted.  Here is another twitter feed that lays out how human creativity is vastly different from AI generated content: https://twitter.com/jkierbel/status/1631538117915156480<br/><br/>We in the WGA&#39;s AI training group knew long ago that ChatGPT could output verbatim material.  But with today&#39;s demonstration of Google&#39;s newest AI system, Gemini, even professionals are realizing that any given AI&#39;s ability fundamentally reflects the contents of its training data. Here is a tweet from the respected AI podcast Machine Learning Street Talk that points out that dependency of an AI model on its training data: &quot;Google pulled this off mostly because of access to DATA, the DATA IS THE MOAT.&quot; https://twitter.com/MLStreetTalk/status/1732437722134954471  Again, what these AI technologies do is a form of COMPRESSING the data, a point which is being acknowledged online in twitter more and more. Here is one tweet from a machine learning entrepreneur: https://twitter.com/ChombaBupe/status/1728796236692484506 -- Journalists are finally starting to pick this up as well: https://venturebeat.com/ai/the-copyright-case-against-ai-art-generators-just-got-stronger-with-more-artists-and-evidence/  And many others are characterizing the creation of these generative AI models as the &quot;greatest art heist in history.&quot; https://artisticinquiry.org/AI-Open-Letter<br/><br/>The way these AIs are trained and commercially deployed is a fundamentally unfair and exploitative practice that hurts artists, actors, writers -- frankly, anyone without a financial stake in the companies behind them. To quote Prof. John Newman: &quot;Widespread, indiscriminate taking for commercial use, without permission or payment, smacks of exploitation. An orthodox economist might call it &ldquo;free riding&rdquo; and a source of &ldquo;market failure.&rdquo; https://lpeproject.org/blog/seven-reactions-to-bidens-executive-order-on-artificial-intelligence/  The harms are real and they are present: artists are losing livelhoods now.<br/><br/>Or to paraphrase Princess Leia in Star Wars: &quot;Help us, Copyright Office. You are our only hope.&quot;<br/><br/> <br/><br/>