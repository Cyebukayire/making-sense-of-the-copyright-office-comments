General:<br/>1. Generative AI has the possibility to give more power to smaller creators and businesses by empowering their workflow efficiency, it&#39;s a huge manpower multiplier in the hands of skilled creators.<br/>4. Other countries in Asia are starting to adopt a stance that training models on copyrighted materials is ok, it&#39;s only the output that matters. Only if an end user produces a work with an AI tool that is essentially a copyrighted work, that&#39;s a copyright violation from the end user of the tool.<br/>5. Probably. I personally agree with the approach mentioned above. Humanity stands to collectively gain a lot from pooling our resources and creativity into one central thing that we can all benefit from.<br/>Training (LLM/Stable Diffusion:<br/>6. Many kinds have been, both purposeful and accidentally.<br/>6.1. Typically gathered from pre-made databases like LAION, or scraped directly from large online databases (shutterstock, danbooru, etc). Notably, these databases don&#39;t necessarily own copyright on the images that they host to begin with.<br/>6.2. Licensed works are being used for some models, such as Adobe&#39;s Firefly. However, the original artists had no possible way of knowing that their work would&#39;ve been used in this way. While it&#39;s legal in this since, the morality of it is no different than training via scraped copyrighted data. <br/>6.4. Training data can be retained in a database for further training, but the released model itself does not have the original data and usually cannot replicate it in a a 1:1 fashion except when the original data is replicated in the training set hundreds of times. However, this is not ideal training and is something to be actively avoided.<br/>7.1. Training material is used by the creator&#39;s of a given AI model when training the model. Original training takes millions of set repetitions which involves significant time and monetary investment. In the case of stable diffusion, the layers of the AI model learn how to reproduce and understand properties of a given image through repetitive training.<br/>7.2. They&#39;re stored as weights and biases in a massive multi-gigabyte table of binary data. They bare no resemblance to the original work in the slightest, and cannot be reverse engineered into the original work.<br/>7.3. No.<br/>7.4. Typically, no. It&#39;s only when a model is &quot;overtrained&quot; on a particular work that it can learn to replicate it more perfectly. If a model was trained on 2 billion images and one particular image was represented 2000 times, it could have the potential of replicating it closer and closer to perfectly, but never quite perfectly.<br/>8.  Using copyrighted work to train AI is entirely transformative, nothing is left of the original data and when correctly, the original data cannot be extracted from a model&#39;s checkpoint.<br/>8.1. The ideal output of a model is to create novel works based off of a learned understanding of old work. The output is significantly different than the original, and the original cannot be directly inferred or obtained from the output. In Andy Warhol Foundation v. Goldsmith the original and the new work were more directly identifiable, but that is not typically the case with AI models where the output is often exceedingly different, except when the end-user outright attempting to produce a copyrighted work.<br/>8.3. Turning around and attempting to sell the model as a commercial buy-in service feels less acceptable than if the model is freely available for all to use. It would be preferable that if non-licensed works are used in the training of a model, than the model should be freely available for the public to use as it made use of public data freely.<br/>8.4. Billions of images. The volume matters in that each individual piece contributes less and less to the finished product. A stable diffusion checkpoint is merely 4-8GB in size, but was trained on billions of images. The original images measured in megabytes, are now represented by mere single bytes of information at most.<br/>9. Opt-Out.<br/>9.1. Only commerical.<br/>9.2. Yes, metadata attached to the image would be fine.<br/>9.3. If we consider ALL copyright holders involved. It is not feasible to get advance consent, let alone figure out who to get consent from.<br/>9.4. New legislation would have to be considered, as a model cannot be realistically untrained.<br/>9.5. No, as they do not own any rights to the work to begin with, and you cannot copyright style.<br/>10. Perhaps a collective library of licensable work for training. Anything else cannot hope to meet the scale of data involved.<br/>10.1. Probably.<br/>10.2. There&#39;s no existing entity that meets the sheer scale of AI data usage.<br/>11. Due to the huge dataset, obtaining a license directly for each image is nigh impossible.<br/>12. No. There&#39;s no way to backwards engineer a particular work in a model, except in the case of extreme overfitting.<br/>13. It would be an untenable burden in the current age.<br/>14. Only the commerciality of the model checkpoint release should impact copyright violations during training.<br/>