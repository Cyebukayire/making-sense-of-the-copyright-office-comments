Copyright was for things created by people, not by machinery which is what AI is. A machine. Also, these AI systems used things &#39;found&#39; on the internet (i.e &#39;stolen) without permission of their original creators. This &#39;found&#39; material includes things that were copyrighted such as the art work &#39;found&#39; on places like Deviantart. Even the people running this AI flat out admit that they felt anything &#39;found&#39; on the internet should be fair to be used by their AIs. This has been extensively reported by many media sources.<br/><br/>Please see this article: https://www.artnews.com/art-in-america/features/midjourney-ai-art-image-generators-lawsuit-1234665579/<br/><br/>The article clearly states why AI is bad for everyone and should not lead to copyrightable material because the basis for all AI, even ChatGPT is stolen ideas and property scraped by their creators from internet sources. Mention by these companies have been made admitting to this in numerous places and news articles.<br/><br/>As a published author I am very concerned by the thought that not only my books which have been heavily pirated across the internet have been used to train these AI, but that the books written by others have also been used. <br/><br/>It is neither right, nor fair, for someone to spend a couple of hours &#39;creating&#39; a piece of &#39;art&#39; or a &#39;book&#39; (often in the style of an actual author or artist) that can then be copyrighted. There have also been instances of actual authors having their real names attached to these &#39;books&#39; that other people used as a way to pretend to be that author. This also has been documented in the news.<br/><br/>Opening up &#39;copyright&#39; to AI created works will not only create easy ways to plagiarize authors and artists, but it will mean creatives will likely see and end to things they&#39;ve spent years, often decades, learning and doing their crafts. Actual artistic creation should not be given over to machinery.<br/><br/>Thank you for your time.