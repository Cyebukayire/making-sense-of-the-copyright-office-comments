In high school, I learned that it was vital to cite my sources, thus crediting the the creator of the idea, statement, artwork, etc. On my part, it was a matter of integrity to do so. Taking credit for other&#39;s work--even if it&#39;s embedded within my own ideas--is contradictory to my desire to be a decent human. However, societally and practically, individual integrity is not the most vital reason for maintaining strict regulations around AI. AI output is entirely created from the work of others. And the people who programmed the AIs exclusively profit from the work of others being regurgitated by their programs. Not only do the original creators (of ideas, art, etc.) *not* profit, they don&#39;t even receive attribution. Short, practical example: a person walks into 10 artists&#39; homes, steals a piece of art from each of them. Then the thief cuts up the original art works and pastes them together, calling it a &quot;new&quot; piece of art. a) the initial act is thievery, and b) the &quot;new&quot; product wouldn&#39;t exist without the stolen artworks. There is zero ethical explanation for allowing this at all, let alone on a global scale. Aside from these basic societal harms, I would add that there is a longer-term harm: disincentivization of creation. Why bother writing a book or creating and posting art when AI firms will steal your work and use their massive infrastructure to market and profit from your work? Becoming a published writer or a notable artist is hard work (consider all of the artists who&#39;ve died before their works became popular and, thus, valuable). It&#39;s already an uphill battle for human ingenuity. AI will dump an iceberg in the heart of that ingenuity, freezing some and crushing others. And, no, I&#39;m not anti-tech. Let the AI companies do what the thief should have done--buy the works of art with full consent from the artists that their work will be used for the &quot;thief&#39;s&quot; personal profit. I&#39;ve no doubt some artists will participate in such a financial opportunity.