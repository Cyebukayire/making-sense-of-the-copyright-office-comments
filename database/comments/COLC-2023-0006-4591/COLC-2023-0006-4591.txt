If youre reading this you probably are already aware the conceptual issues of the actual technology, and we&#39;re only beginning to unearth the moral and long term issues. I could go for hours on this but I won&#39;t. Instead I&#39;d like to talk about about coordinated vulnerability disclosure (CVD) and full disclosure (FD). As a hypothetical example when a company or entity like HP finds some security issue under CVD they would gather a team to fix and study that issue, then once remedied would disclose it so that all other individuals or companies dont fall into the same mistake or could get help out of it. FD is when a car company like telsa has an issue with their car and tells everyone. That way the consumer can bring their car in, maybe avoid the things that make that issue more likely to occur. With any hope FD eventually results in a fix. Both are good. Some occasions one is better than the other. Keeping these issues private or sweeping under the rug is bad for everyone. Where does this relate to AI?  This starts with the &quot;no moat&quot; issue. A few months back a Google researcher released an internal memo detailing that Google is not at the forefront of AI development and that third parties, entities and individuals seperate from silicon Valley largely held many of the cards. That the algorithms and models used to train said algorithms were open to the public and anyone who could read and comprehend it had access. I thought this was hyperbole, so I bought a decent laptop from Costco under $1000 and tested it using stable diffusion, an image generation software. It took some time and learning but they were ultimately correct. More importantly this was not the same as using some website/service like midjourney to generate art, this was all mine, no moderation, no rules. I could create my own models, tweak the existing algorithm to my specifications no problem. Want a digital painting of the green M&amp;M climbing mount everest in a Sombrero? Give me a few hours and it&#39;s yours faster than I&#39;d ever be able to do on my own otherwise. What I&#39;m getting at is that this technology for better or worse can be in the hands of any schmuck above the age of 13. Genie is out of the bottle, so what now? Well I&#39;d argue we need to mandate a disclosure model. Maybe dependant on scenario and possible harm. No government has the power to snuff every bad actor everywhere, however I do believe other nations also have a shared goal in keeping tabs on possible threats. As you&#39;re likely aware the threats could be coming from all sides. Businesses, hobbyists like myself, foreign governments, anyone with a thousand bucks and a costco membership. What I&#39;d argue is a central body to require full appropriate disclosure of those using and developing these natural language processing and neural network algorithms and implementing them. If a company creates a psychological treatment app, some ai therapist, I think The public needs to see triple blind trials. If a state is using some level of facial recognition to try to identify an ethnic or religious they aren&#39;t fond of I think its people should know. If a fast food chain is looking to replace some workers wouldn&#39;t it be the least they could do to give notice? If an individual wants to create some algorithm that finds the most effective phishing templates wouldn&#39;t it be better for the public (and authorities) to know and look out for that? Like I mentioned before there&#39;s no undoing this kind of technological advancement. There are however ways to attempt to be more transparent in good faith. Call it naivet&eacute; but I believe if an effort is made on all sides internationally we could all move towards a future that minimizes whatever kind of self harm we&#39;re about to inflict.