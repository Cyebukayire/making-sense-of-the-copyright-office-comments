1. The risks of this system is far greater then the rewards the way AI works right now. Individuals are taking the voices of the living and deceased without their consent and creating dialogue that could cast those stolen from in a bad or dangerous light. The only AI that has been a benefit is the AI created in house and by the team who created Spider-man Across the Spider Verse. They created it in house, with their own personally created data the teams designed for it and used it as a tool and not a means of theft.<br/><br/>7. The way AI tools gain training is they scrape sites for images ,and take labor from human individuals who do not consent to this, to train their models off of to then attempt a gain in profit from stolen work they&#39;re trying to pass of as their own labor. This is also being done the exact same with written works. The online archive writing website Ao3 has been dealing with individuals scrapping the site to make a profit off of stolen labor.<br/><br/>9.  Opt Out should be the default across the board on all websites, programs, etc.. Be they for profit, non-profit or research. Individuals and companies can not be trusted to with any but Opt. Out as default. It&#39;s deceitful conduct and puts the burden on those who are being stolen from in stead of on the people/businesses that benefit from theft with a default Opt In model. Morally it is also imperative to have Opt Out as default to protect those who do not have the means or understanding of AI or are simply deceased.<br/><br/>9.5. should they have a right to object to an AI model being trained on their work? Yes. If an individual is hired or assigned a project and it was not discussed or consented to then their work should not be used for training. It is in the same vein as those who buy a product from a small business to then turn around a base their knockoff product off of to then sell the stolen work via mass production.<br/><br/>15. Full transparency must be had. If they&#39;re going to have a database of people&#39;s work they&#39;ve taken then the public must be allowed to know know what has been used.<br/>15.1. Location taken from, username/creators public facing identity, date the source was created, date added into database, etc.<br/>15.2. Anyone who requests it. To allow transparency and trust, any individual should be allowed to make an inquiry to ensure they have not be stolen from.<br/><br/>16. Full obligation. If you use it, you must inform the owner of the original works. Owners of the AI and all trainers who use the AI MUST inform the original works owners.<br/><br/>18. The only time it should be able to be copyrighted is when it&#39;s developed in house and train on in house material created specifically for it&#39;s use and intended purpose. Refer to Spider-Man Across the Spider-verse and the AI they created and trained.<br/><br/>28. Yes. All works should at minimum have some easily identifiable indicator such as a visible watermark/stamp on it to inform the public that it is AI. It&#39;s a moral and legal duty for all AI to state it is AI, as we already have people doing deep-fake voices and visuals of not only famous people but political leaders. This is a dangerous tool if it&#39;s not forced to be easily identifiable for the public. People&#39;s entire lives can be ruined, as deep fakes of an adult nature targeting people for revenge have already shown over the last few years.<br/><br/>32. Yes there should be protections from machines training off and imitating an individual/businesses style. An individual training their eyes and hands to be able to imitate a style takes years of work and effort and is expected in the industry for collaborative efforts. Machines just takes data and outputs images with no human element or skill.<br/><br/>Who should be eligible for such protection? Any individual/business who does not consent to a machine learning to imitate their &quot;style&quot; and works. If the protection is not across the board then we run the risk of bad actors imitating individual/business in bad faith and presenting works that could damage reputations and livelihoods.<br/><br/>What form should it take? Permanent Opt-outs for any and all data models. Require consent from individuals/business to have a dataset trained off their work, and it must be by the individual work piece. No sweeping consent, must be per-piece trained from. Include right to revoke access to work and removal of creations from the datasets if the original owner of the work believes foul play or misrepresentation is afoot.