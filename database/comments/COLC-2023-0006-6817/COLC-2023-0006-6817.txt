When it comes to the sourcing and creation of &ldquo;AI&rdquo; generated images, audio, or text, no material about or belonging to another person should be used to train any system without that person&#39;s explicit, affirmative, informed consent. There is a wide array of available imagery and written information that we can make use of without impinging on the rights of another. Indeed, many people would be happy to provide their work to such an endeavor, as long as their contribution was compensated and that consent could be revoked at any time.<br/><br/>To that end, any extant &ldquo;AI&rdquo; models trained on unethically sourced materials should be immediately ceased, the datasets scrubbed, and training redone under newer, more ethical, just, and energy-efficient frameworks.<br/><br/>Unless and until such time as a chatbot or other large language model-based system autonomously and without human direction successfully completes an application for copyright, any copyrighted material must be the product of *Significant and Fairly Compensated Human Authorship*.<br/><br/>Next, every generative &ldquo;AI&rdquo; system must be required to catalog the provenance of all their training data, in much the same way as museum catalogs track the history of their art, or academic researchers cite their sources in a bibliography. In addition to the watermarking of any &ldquo;AI&rdquo; illustration, video, or audio, as listed above, all GPT-type systems must clearly state that they do not provide the user with &ldquo;truth&rdquo; or &ldquo;facts,&rdquo; but rather with a collection of words which it has determined to be statistically more likely to match the users&rsquo; inputs.<br/><br/>Tools to examine and parse digital artifacts in generated audio or video already exist, and can be expanded upon and deployed in a user experience environment which capitalizes on pre-bunking and other misinformation mitigation strategies. In this way we can teach people how to be more critical and reflective of both the information they take in and their own guiding values and biases. This can be paired with interventions such as encrypted signage of posts, texts, tweets, videos, or audio of government officials. These can be steganographically integrated into whatever medium as a watermark and used to verify the identity of the official. Such a practice would be onerous for the everyday user, but easily implemented within government agencies and corporations. However, great care would have to be taken to safeguard the integrity of those cryptographic keys.<br/><br/>These steps would allow people to immediately determine whether their work has been included without their permission, rather than having to rely on third-party searches or public disclosures from the discovery phase of legal action. Any future company found to have violated these principles or created other lasting harm should see an immediate halt to their work, an oversight review and audit by the above-described regulatory body, and a potential restructuring. The datasets themselves should, again, be completely scrubbed, and any deployed product using them pulled from production and public use. This all also helps foster a real transparency, knowability, and accountability of these systems.<br/><br/>Building these tools differently will require building new perspectives, intentions and beliefs into the creation and regulation of &ldquo;AI,&rdquo; which will require changes to the entire culture of which that technology is a part. Designers and creators will need to learn to be forethoughtful about the potential harms, to recognise and accept when it is not possible to reform a tool or system, and the willingness to instead consider abolishing that tool or system, or even dismantling a parent company. To achieve this, we must value and center experts with deep knowledge of how science, technology, ethics, justice, belief, and human values all intersect&mdash; experts who very often happen to be among the most marginalized and disregarded, when it comes to the truth of their own lived experience.<br/> <br/>Without a commitment to recognizing and empowering the forms of expertise that come from an entangled and interdisciplinary program of knowledge which includes lived experience, we&rsquo;re likely to continue making &ldquo;AI&rdquo; which reflects only the values we unconsciously and accidentally embed into it, rather the ones we&rsquo;d actually prefer.<br/><br/>Supporting Documentation:<br/>&ldquo;Bias Optimizers&rdquo; <br/>https://www.americanscientist.org/article/bias-optimizers <br/><br/>Belief, Values, Bias, &amp; Agency: Development of &amp; Entanglement with &ldquo;Artificial Intelligence&rdquo;<br/>https://vtechworks.lib.vt.edu/handle/10919/111528<br/><br/>&ldquo;The Ethics of Artificial Intelligence-Generated Art&rdquo;<br/>https://sinews.siam.org/Details-Page/the-ethics-of-artificial-intelligence-generated-art 