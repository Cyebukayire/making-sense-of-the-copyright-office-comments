#5: My general legislative advice would be to regulate/prohibit uses that are inherently defamatory or fraudulent (deepfake imagery of individuals in a compromising situation, financial fraud, identity theft, etc.). I would also provide higher protections for parts of an individual&#39;s identity (voice and likeness, but not an artistic or musical style) than other content, though whether the individual is a public figure would also have to be taken into consideration (for example, an individual should probably get compensated if an AI version of their voice or image were used in a fiction-based movie or song, but it should probably be acceptable for use when reading off, say, a court transcript*, or some other factual context like a documentary, as long as the use of AI is disclosed).<br/><br/>*Not your area, but as I wrote this I realized there&#39;s also the possibility of using an AI voice in court-definite 5th Amendment question there.<br/><br/>#7.3 You can generally remove an element from the training data and then continue to train the model further, but that training will start from a point where the removed element had some influence, which will continue to decline as the model continues to be trained. You could say that the removed data&#39;s influence is effectively eliminated at some point because computers have a finite level of numerical precision, but even then you&#39;ll effectively have a different model than if the item had never been in there.<br/><br/>#7.4 Except in cases where an inordinate amount of a particular piece of training data is over-represented (commonly referred to as overfitting, which is considered a bug), no.<br/><br/>#8 In general, I think it should qualify as fair use. Based on my understanding, the way model training works is that each piece of data is repeatedly analyzed, and the model is trying to work out some sort of statistical analysis based on all the relevant pieces of training data that works for its intended use case. For example, a large image training set might have tens of thousands of pictures of apples, all of which contribute to the model&#39;s understanding of the size, shape, color, etc. of apples, but because the model only devotes a small portion of the model to data about apples (kilobytes of data versus a multi-gigabyte model file), it has to aggregate that data and the result is usually a summary of what that particular thing can look like. While each image included does tend to make the understanding more complete, the contribution of an individual image is small, thus lending (in my view) a strong case for arguing the use is de minimis. I realize folks don&#39;t like being considered part of a statistic, but that&#39;s kind of how it works out, even with just a few dozen samples.<br/><br/>#8.1 I&#39;m not a lawyer, but neither case seems to offer much guidance. As I noted in my previous answer, the impact of individual works on the output of the model is quite small, as opposed to the Warhol case where a single photo served as the basis. The Oracle case seems like it would be more relevant if someone was trying to build a model to interface with another AI architecture.<br/><br/>#8.5 In general, I think that only the outputs of the model competing with a specific work really needs to be considered. &quot;General class of works&quot; seems way too broad to me, and I think &quot;works by the author&quot; is akin to copyrighting a style, and since an artist&#39;s style changes over time, violates the &quot;fixed in a tangible medium&quot; principle.<br/><br/>#9 While I think model creators should be able to implement an opt in or opt out structure if they desire, I think legally requiring one is a bad idea because A. I think that generative AI will be useful for the press in speculative media (covering possible casting choices for a movie, discussing alternative endings, illustrating possible future events), and I don&#39;t like the idea of an artist (or more likely, a company) putting their work &quot;off limits&quot; for that kind of reporting. As long as the use of AI is disclosed there, I think it should generally be allowed in that context, and I&#39;ve seen several examples of it being used that way already. B. A lot of the discussion of this topic comes from the perspective of a large tech company and its relationship with independent artists. However, models created by small groups or individuals are quite common (and becoming more numerous, of course), and if a legally required right to opt-in/out is present, the general public may largely respect the wishes of individual artists, but I think there&#39;s a decent possibility that similar requests from large corporate entities will be met with widespread civil disobedience. This could result in a situation like what we saw in the early 2000&#39;s with the music industry where a significant portion of the public appeared to deem the industry&#39;s prices and format offerings insufficient, and it could be even bigger with AI if they interpret these models and frameworks as being sufficiently removed from the source material.