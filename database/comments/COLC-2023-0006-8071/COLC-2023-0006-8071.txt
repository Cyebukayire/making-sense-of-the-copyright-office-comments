The manner in which AI is currently allowed to scrape without consent is immoral and can be used to compete with the artist whose works were fed into the algorithm.<br/><br/>Have a look at CivitAI: a site for open source AI which has been frequently used to generate &quot;LoRA&quot; (Low Rank Adaptation) models. These LoRAs are often used to fine-tune to imitate specific artists. Examples of this would be SamDoesArts, WLOP and Greg Rutkowski LoRAs littered across the internet. SamDoesArts and Greg Rutkowski have repeatedly requested being taken down these generators only for them to be reinstated.<br/>These models allow impersonation to become incredibly easy, convenient, and use the artists&#39; own work non-consentingly in order to compete with their own market.<br/><br/>There is no longer incentive to develop a niche or novel product let alone upload them to the internet as mass replication of works strongly similar to your own is the way this current technology is headed. Machines and humans are clearly distinct entities, where the former is far more capable of imitation, impersonation and mass generation. They should not be treated the same. &quot;Humans and AI learn the same way&quot; is not at all valid. Refer to the distorted Getty Images watermarks found in AI generated images. A human would not do this.<br/><br/>This is not an argument for copyrighting &quot;artistic style&quot;. This is an argument for algorithms to be disgorged and obtain their training material responsibly with consent. Arguing for being &quot;transformative&quot; is an attempt to steer the discussion away from the fact that it is using non-consensually obtained training material that can be used against the artist whose works were ingested. It is unfair to not let the artist or creator get a say what happens to their works when the algorithm is not merely curating their works like Google, but using it to generate like products to compete.<br/><br/>As it stands, artists and creators have absolutely no incentive to post their works online. They have no protections whatsoever and can barely claim ownership over their own works, with this technology allowing impersonation to the extreme. Opinionatedly, this is extremely bad practise done by a company attempting to save on costs, releasing their software to open source to strong-arm the law and &quot;ask for forgiveness, rather than permission&quot; (Jim O&#39;Shaughnessy, Executive Chair of Stability AI). We should not allow this precedent to be established.This technology could have been developed in a manner that benefitted all parties, such as being paid for training material or simply being opt-in in the first place.<br/><br/>Methods such as &quot;opt-out&quot; are not at all fair. Companies can hide the fact that they are at all opted into the algorithm or hide the opt-out button to conveniently allow for more training data. Additionally, it should not be on the creator to have to scourge every single algorithm (e.g. Facebook, Twitter, Deviantart, etc.) and select &quot;opt-out&quot; to each of them. If they want to be opted in, show them an opt-in option and let them decide for themselves.