I have worked as a digital artist for 15 years, doing 3D modeling and rendering for a living.  The content I produce is sold through a marketplace called Daz3D.<br/><br/>Over the last year I have been following AI, it&#39;s impact on the industry and myself.  I have found out that the image datasets used to train models for Stable Diffusion, Dall-E and others contain a considerable amount of my rendered artwork, along with work done by my friends and colleagues.  All of which obtained without permission.  This is just one dataset and I know there are others.<br/><br/>https://laion.ai/blog/laion-400-open-dataset/<br/><br/>These models trained in part on content I produced have been used to promote image generators like Stable Diffusion which directly compete with my customer base.  The images were taken without permission and used to create something that circumvents many of the reasons to purchase my content.  Users can add &quot;Daz3D&quot; do their prompts and produce derivative works because it was trained specifically to work this way.<br/><br/>Throughout the last year as I&#39;ve watched my sales metrics I&#39;ve noticed a sharp decrease in profit.  The incentive for customers to support my livelihood isn&#39;t there when they can produce similar content for free, using these models based on my work.  Companies like Stability AI are profiting off of my years of effort and I am left with no recourse because these models have already been trained and distributed.<br/><br/>Through all of this I have also watched the AI community and witnessed their conduct.  Much of what I see is a sentiment of entitlement, the thought they have the right to artist content without restriction.  I&#39;ve seen many instances of artists denouncing AI only to have the community purposely train derivative models on their work, purely out of spite or retaliation.  Fellow artists are getting harassed for calling out the industry that is destroying their livelihood.  There is a significant amount of mocking and boasting, all around an industry that would not be functional without our unwilling support.<br/><br/>The most current discussion is surrounding a software concept known as Nightshade that aims to allow artists to protect their work from AI training.  Talk around the AI community is denouncing this protection, calling it&#39;s ability to disrupt training unrightful.  Some users have expressed desire for retaliation, discussing the prospect of DDOS attacking the project and ways to circumvent the protection.  This all further exemplifies the idea that they feel entitled to the content of artists and wish to do with it what they want without restriction.<br/><br/>My suggestion is to require the AI industry to work on an opt-in basis.  Opt-out has been proven not to work, it&#39;s often done so in a way that purposely makes it difficult.  Companies with opt-out standards have required users to do so for every individual piece of work produced which is knowingly time consuming.  At this point opt-out only applies to future models as well, not those currently being train or those that have already been trained.  An opt-in approach would preferably current training and require revisions for current datasets, and in the best case scenario regulate models already trained on ill-gotten content.<br/><br/>I would also suggest requirements for disclaiming AI generated content.  Many people do not want to support the AI industry and going forward it will only be harder to tell what has been produced this way.  Requirements for disclaimers would give consumers the ability to choose whether to support the content or not.<br/><br/>Thank you.