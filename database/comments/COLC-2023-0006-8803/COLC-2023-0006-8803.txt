Probabilities, tokens, and liability. <br/>The U.S. should consider relaxing copyright infringement principles for Large Language Model (&ldquo;LLM&rdquo;) predictions.<br/><br/>This memo responds to the U.S. Copyright Office&rsquo;s August 30, 2023 Request for Inquiry on Artificial Intelligence and Copyright. <br/>Setting. <br/>This memo is through the looking glass of a company molding an LLM to supply the most meaningful and relevant information safely and responsibly for its employees and customers.<br/>The company steers &ndash; with Retrieval Augmented Generation (&ldquo;RAG&rdquo;) and by adjusting model parameters (&ldquo;Fine Tuning&rdquo;) - a closed source publicly available pre-trained LLM (&ldquo;foundation model&rdquo;) that it accesses through the company&rsquo;s own copy of the foundation model&rsquo;s API. The company fills its RAG knowledge base and data source for fine tuning with its own private content or content they have licensed.<br/>A pre-trained LLM is a massive math equation making predictions. It creates text from a user&rsquo;s instructions by guessing the next string of characters - represented as a vector of numbers &ndash; sequentially picking the vector of numbers it assigns the highest probabilities to (using its architecture of the number of &ldquo;layers&rdquo;, number of &ldquo;neurons&rdquo;, and forms of &ldquo;activation functions&rdquo;; and parameters, the numerical values of its &ldquo;weights&rdquo; and &ldquo;biases&rdquo;). The LLM calculates its weights and biases through mathematical equations (&ldquo;optimization&rdquo;), for a given architecture and a vector representation of its training data. <br/>With RAG, the company uses search techniques and the foundation model&rsquo;s architecture and parameters to package information from the company&rsquo;s knowledge base of private content or licensed content to the user based on their instructions. When the company fine tunes, it uses its own private or licensed data to change some of the foundation model&rsquo;s parameters.<br/><br/>Dialogue. <br/>The Copyright Office&rsquo;s Notice of Inquiry states &ldquo;the Office is interested in how copyright liability principles could apply to material created by generative AI systems. .For example, if an output is found to be substantially similar to a copyrighted work that was part of the training dataset, and the use does not qualify as fair, how should liability be apportioned between the user whose instructions prompted the output and developers of the system and dataset?&rdquo;<br/>We believe that companies using foundational models with RAG should not bear liability if private or licensed content fills the company&rsquo;s knowledge base. With RAG, a foundational model returns information only from the company&rsquo;s knowledge base. The foundational model&rsquo;s weights just help pick what sections of the knowledge base to use in its response; how to use the knowledge base sections in its response; and to send output in relevant English &ndash; for example output that matches the company&rsquo;s branding, that has good grammar, and responds to how the user wants to structure the output. <br/>It is more complicated when a company fine tunes a closed source foundational model. Questions you will need to consider that we do not have an answer for: <br/>1. Is there a minimum number of foundational model parameters that fine tuning using private and licensed information needs to change for a company to not infringe on copyrighted material that a foundational model may have trained on?<br/>2. If the foundational model is closed source, how can a company specify what layers of the model they want to freeze, that will influence the number of parameters they will change with fine tuning?<br/>3. If the foundational model is closed source and a company cannot specify how many parameters they want to change, is a proxy instead the amount of data they use for fine tuning?  <br/> <br/><br/> <br/><br/>