7.1 When a model trains on some particular material, it uses machine learning to gain a marginally better understanding of what the material contains, and then moves on. It does not store that data anywhere- what it actually takes away from the experience is some change in the weights that it puts to certain internal variables. As such, each training material that it consumes informs its &#39;ideas&#39; as to what some final product might look like, but since it does not store material, it can&#39;t exactly reproduce the material. People have noted online in the case of AI art that it can sometimes produce watermarks that clue into what it was trained on, and claims that this means that AI art is really just mashing pictures together. It produces these not because it is ripping images from the training data, but because it was convinced by that training data that images similar to the ones requested to be generated should have that pattern. If you suspend disbelief for a moment, and you showed a human only pictures of mountains with a watermark for their entire life, you might be able to eventually convince them that the watermark is part of what makes a mountain a mountain. Then if you asked them to draw a mountain, they would probably include that watermark, even without any sort of reference copy to look at or even any particular image in mind. It is the same process here, except that the machine is actually capable of believing that, with no suspension of disbelief needed. As for duration of use, machines need a lot of data to do any learning and the sheer volume of that data necessitates that no individual piece be analyzed for any significant amount of time- or at least any amount of time that would be relevant to a human, supposing duration is a sticking point for them rather than whether or not analysis should be allowed to begin with. <br/><br/>8.4 While I am unsure as to the typical specific amount, it is a lot. More than a human could analyze in a lifetime. Popular sources on google claim that chatGPT was trained on 300 billion words. Reading at 600 wpm (10 words per second) that would take 30 billion seconds, which is more than 900 straight years of reading. The volume should not affect the fair use analysis- why would performing many legal acts constitute an illegal act? If anything, the volume being as large as it is should indicates that any individual copyrighted work can hardly claim to be responsibility for any non-negligible portion of the final result.<br/><br/>9. If anything at all, they should be provided with the means to opt out. <br/><br/>9.2 By my understanding, if there were consensus that it was necessary, it would not be too difficult to add a metadata field to certain file formats that would indicate opting out. However, if the data were to be copied around or converted to different file formats or screenshotted by a fan and posted somewhere else or any number of things, it could eventually end up being part of a dataset anyway. The only way I can think of to be totally sure that it did not enter any datasets would be to keep the work to oneself, or at least not allow it to enter the internet. Unless we are to assume that automated aggregating services would &#39;be good&#39; and only take in images with an approved file type and the metadata filled out. In that case I believe it should work.<br/><br/>9.5 No, it&#39;s not even theirs to begin with.<br/><br/>28. It is essential that any AI generated works that could be confused with reality, especially deepfakes, be consistently and clearly labeled as AI generated. Ideally, this would include indicators that cannot be removed, even with ill intent. It will be a bad day indeed that video can no longer be trusted when it comes to figures who are important in the public consciousness, especially regarding politicians. Both because real video could be claimed fake and fake video could be claimed real. This is important not just for the landscape of politics, but for the lives of the average American as well. As the technology improves and access becomes faster and easier, it is inevitable that people will create AI videos of people in their lives. Sometimes this may occur as an in joke between friends, but sometimes it will occur as a means to hurt someone. Necessitating that such videos be clearly labeled as fake will avert much grief in the future and is vital to the wellbeing of everyone in the country. Clearly labeling such things should be dealt with firmly, and strictly.<br/><br/>30. People should be protected from commercial usage of any such materials. As mentioned in 28, they should be clearly labeled as fake, but so long as that is adhered to there is little issue with noncommercial productions mimicking one&#39;s likeness or vocal likeness. But no one should be making money off of someone else&#39;s likeness without their consent, no matter how they go about it.<br/><br/>31. Yes. See 30.<br/><br/>32. No, there should not be. Styles should be uncopyrightable.