The output of recent artificial intelligence models&mdash;whether images or text&mdash;are transformative works, and I urge you to listen to the engineers developing them for the explanation of why. The people who lead the moral panic about artificial intelligence think that the inclusion of a given piece of artwork or text in a neural network training dataset puts it into a massive internal library that the AI directly samples and remixes into an output filled with copyright violations. In truth, however, neural networks learn how to form their output similar to the way a human learns.<br/><br/>The training allows the neural network to look at millions of example images or text, and form artificial neural connections between input and the expected result. The individual rules it learns cannot be expressed in human language, but the emergent behavior is that an art-generating AI learns how to arrange pixels to visualize different requests such as a &ldquo;portrait&rdquo;, a &ldquo;fish&rdquo; or an &ldquo;impressionist painting&rdquo;, while a Large Language Model like chatGPT learns how to respond to requests such as &ldquo;a list of instructions to bake a cake&rdquo; versus &ldquo;a short story about an invisible superhero&rdquo;.<br/><br/>One major argument from the people who fear AI is that it&rsquo;s a &ldquo;lossy&rdquo; storage method for the input images or text it&rsquo;s trained on, but a little math proves these arguments wrong. A stable diffusion checkpoint is 2 GB and was trained on about 160 million images, which means that there remains about 13 bytes of information per image. Even the smallest images on the internet are typically measured in kilobytes, hundreds of times that size and more often millions of times the size. There is no possible lossy algorithm that can reduce an image to a mere 13 bytes and then meaningfully restore it to its original size.<br/><br/>This brings me to my final point. The objectors to AI argue that the ability of diffusion models to generate copyright-infringing images such as the Afghan Girl or Mickey Mouse prove that artificial intelligence stores copyrighted IP and infringes on the creators&rsquo; rights, but that&rsquo;s still a misunderstanding of how the technology works. The more often a subject appears in a training set, the better an image generator is able to reliably replicate it. Hundreds of identical images of the Afghan Girl can be found on the internet and by extension in stable diffusion&rsquo;s training set, so stable diffusion learned how to closely replicate the reds, greens, folds, and shading of the original photograph. In contrast, Mickey Mouse can be reproduced in a wide variety of poses and scenes due to the variety of images of the character on the internet, though this variety makes stable diffusion unlikely to identically produce any single image from the input. While in either of these cases the output violates copyright, the same can be said if someone traced an image of Mickey Mouse, or took a picture of the Afghan Girl and tried to call it their own; it boils down to the intention of the user. <br/><br/>While art-generating AI is capable of generating copyrighted images, on the whole it produces transformative works unless you are deliberately trying to replicate a copyrighted IP. Because neural networks do not store a dataset, the inclusion of copyrighted works in the training dataset should be considered fair use. The same all holds for language models such as ChatGPT.